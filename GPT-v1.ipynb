{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb77d17-c4a0-40a3-98f7-b9ec53cdc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afccc6e1-caf5-4802-a029-6be1fca94698",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec2a12e-869a-43dc-9487-45157b13b18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 64\n",
    "max_iters = 500          \n",
    "learning_rate = 3e-4\n",
    "eval_iters = 50        \n",
    "n_embd = 128             \n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "dropout = 0.2\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b98af1-6132-4f60-8d76-69768d921411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', '(', ')', ',', '-', '.', '0', '1', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24c0d0b-7de0-4a78-81be-3d20ea2b55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521b921f-5e0a-4662-a9bf-1a62483ab5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 166,239 tokens | Val: 41,560 tokens\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Train: {len(train_data):,} tokens | Val: {len(val_data):,} tokens\")\n",
    "\n",
    "x_train = train_data[:block_size]\n",
    "y_train = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e1eee5-b7b6-49df-8800-b8bc1d08d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[46, 55, 58,  ..., 42, 45, 45],\n",
      "        [47, 41, 62,  ..., 45, 59, 49],\n",
      "        [48, 45,  1,  ...,  0, 59, 48],\n",
      "        ...,\n",
      "        [33, 55,  6,  ..., 42, 55, 52],\n",
      "        [55, 61, 59,  ..., 60, 55, 55],\n",
      "        [53,  1, 48,  ...,  1, 59, 43]])\n",
      "targets:\n",
      "tensor([[55, 58,  1,  ..., 45, 45, 59],\n",
      "        [41, 62, 45,  ..., 59, 49, 58],\n",
      "        [45,  1, 59,  ..., 59, 48, 45],\n",
      "        ...,\n",
      "        [55,  6,  1,  ..., 55, 52, 44],\n",
      "        [61, 59, 52,  ..., 55, 55, 44],\n",
      "        [ 1, 48, 41,  ..., 59, 43, 58]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device) \n",
    "    \n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80012a62-648b-4b6e-a98a-727e9f94555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77bc826-414f-47ea-a401-f42bff251f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_embd = how many numbers represent one word\n",
    "# n_head = number of attention heads - how many different perspectives does the model see\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \"\"\"\n",
    "        head_size calculates how many numbers each attention head considers - data is segmented for \n",
    "        different perspectives\n",
    "    \n",
    "        Multi head attention helps understand the meaning of words in context. This is done by creating 3\n",
    "        words for each vector - key, query, value. Calculation: dot product of key and query. If word A's\n",
    "        Query matches word B's Key, they get a high score. score is turned into a percentage (softmax). \n",
    "        Model then multiplies the percentage by the values.\n",
    "    \n",
    "        Normalise data after the attention comnputation (self.ln1)\n",
    "    \n",
    "        Feed forward processes the data gathered from the multi head attention phase using a two layer\n",
    "        neural network. data is multiplied by a weight matrix + bias, then an activation function is \n",
    "        applied to introduce non-linearity (e.g. ReLU), then multiplied by a second weight matrix\n",
    "    \n",
    "        Normalise data after the feed forward computation (self.ln2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):     \n",
    "        \"\"\"\n",
    "        pre-norm transformer architecture (laptop limited to CPU computation)\n",
    "        \n",
    "        steps 1-3: multi head attention\n",
    "        steps 4-6: feed forward\n",
    "        \"\"\"        \n",
    "        # 1. apply normnalisation to input\n",
    "        norm_x1 = self.ln1(x)        \n",
    "        # 2. Get the attention patterns from the normalized data (important words have high score)\n",
    "        attention_out = self.sa(norm_x1)        \n",
    "        # 3. Add the result back to the original highway (Residual connection)\n",
    "        x = x + attention_out     \n",
    "        # 4. apply normalisation to feed-forward input\n",
    "        norm_x2 = self.ln2(x)        \n",
    "        # 5. Let the model 'think' about the context (feed forward)\n",
    "        ffwd_out = self.ffwd(norm_x2)        \n",
    "        # 6. Add that back to the highway\n",
    "        x = x + ffwd_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef2eeaa-bbf5-4ff7-85ca-ec97a7d26c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        \"\"\"\n",
    "        self.net creates a container that runs layers in a fixed order\n",
    "\n",
    "        nn.Linear(n_embd, 4*n_embd) projects the word vector (n_embd) into a larger space, \n",
    "        giving the model more 'room' to learn complex patterns\n",
    "\n",
    "        ReLU turns negative values into 0, introducing non-linearity\n",
    "\n",
    "        squash larger (4*n_embd) vector back into n_embd size, so it can be added back\n",
    "        to the highway (original features)\n",
    "\n",
    "        nn.Dropout causes some neurons (nodes) to be randomly ignored in training so the nn does\n",
    "        not overfit\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd,n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e93562-3094-4c34-9c2a-6847d17e66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\"\n",
    "        self.heads intialises a pytorch list with random weights ready to be updated\n",
    "        \n",
    "        self.proj is not always required as head_size * num_heads == n_embd. This line is good\n",
    "        practise if you want to change the parameters, and also introduces a bias term as a learnable\n",
    "        parameter for the neural network\n",
    "        \n",
    "        self.dropout randomly sets 20% of nodes equal to zero        \n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        loop through input data, getting an answer for each input, then concatenate all the 'answers' together\n",
    "\n",
    "        20% of the answers are removed. If the nn is robust, the same outcome should show even with 20% missing\n",
    "        \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, C) -> (B,T, [h1, h \n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7049bd-bad8-4b26-9799-d130adda7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    batch - number of samples being processed e.g. 32 sentences\n",
    "    time-step - how many words in each sentence\n",
    "    channel - how many features per word (384, compressed to 48 - head_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        \"\"\"\n",
    "        different weight matricies and intialised by key, query and value\n",
    "\n",
    "        two seperate weight networks turn the input into a key and query. Key and query vectors are calculated\n",
    "        by multiplying the token features by their respective weighting network. Key and query mutiplied together \n",
    "        (along with the value) to get a score\n",
    "        \n",
    "        nn.Linear(n_embd, head_size) compresses vectors so its smaller and easier to compute with. This is similar\n",
    "        to feature extraction. Out of 384 features, pick 48 that are most influential.\n",
    "\n",
    "        key represents vector notation of what the word represents\n",
    "\n",
    "        query represents what the token is looking for, if key includes token about 'banana' query would be looking for \n",
    "        some sort of food word\n",
    "\n",
    "        if key and query are similar their dot product will be higher. This gives the \"attention score\". \n",
    "        example: \"the cat sat down\" - the word 'the' is not relevant so has a low score (2%) whereas 'cat'\n",
    "        has a high score (80%), so when we multiply by the input value tokens, we take 80% of the 'cat' token and\n",
    "        2% of the 'the' token.    \n",
    "\n",
    "        trill - traingle lower. prevents the model from looking into the future, attention scores only on previous words\n",
    "        register_buffer means model weightings do not update automatically, enforicng model from not looking at next \n",
    "        words.\n",
    "\n",
    "        dropout randomly deactivates 20% of neurons within a layer, preventing overfitting \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input - (batch, time-step, channel)\n",
    "        output -    (batch, time-step, head size)\n",
    "\n",
    "        attenton scores are compued by doing dot product of relevant matricies, which\n",
    "        is a common method to measure similarity\n",
    "\n",
    "        matrix multiplication requires matricies to be the correct dimensionality, so \n",
    "        the transpose of matricies is used to turn (T * head_size) into (head_size * T)\n",
    "\n",
    "        .shape[-1]**-0.5 scales the values so they are not too big/large by dividing by \n",
    "        the square root of the last dimension of the vector (head_size)\n",
    "\n",
    "        fill 0 values from the matrix multiplication with negative infinity, which enforces\n",
    "        probability of 0 when the softmax function is applied (no 'cheating' looking into the future)\n",
    "\n",
    "        softmax dim=-1 ensures the probabilities within a sentence add to one\n",
    "\n",
    "        dropout prevents overfitting\n",
    "\n",
    "        output is a blend of the input vector multiplied by the attention (weights) placed on each token (word)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "\n",
    "        # compute attention scores - use transpose so the matrix dimensions allow multiplication\n",
    "        \n",
    "\n",
    "        weights = q @ k.transpose(-2,-1) * x.shape[-1]**-0.5 # (B,T,hs) @ (B,hs,T) -> (B,T,T)\n",
    "        weights = weights.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) # (B,T,T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B,T,T)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        out = weights @ v\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1c8688-7944-4c77-b17d-636fa2719cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 620,873\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        map token IDs to vectors, getting a vector size n_emb for each token. This is how \n",
    "        each character is stored in the 'brain' of the nn e.g. 'z' = [0.3, 0.6, 4, 6] etc.\n",
    "        \n",
    "        positional embedding of tokens - where each token is in the sentence\n",
    "        \n",
    "        stack transformer blocks - \"thinking\" part that builds context\n",
    "        \n",
    "        normalise final hidden representations\n",
    "        \n",
    "        take previous calculations (context) and calculate logits for every \n",
    "        possible letter in vocab_size. Logits can be turned into probabilities with softmax later\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) \n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights) # random weights assigned initially\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        isinstance - Checks if the current layer is a 'Linear' (thinking) layer\n",
    "\n",
    "        layer weights are filled with small random numbers that are normally disctributed, mean=0, std=0.02\n",
    "\n",
    "        set bias terms equal to zero, so you calculate weights only - estimating weights and bias at the same time\n",
    "        would cause confusion\n",
    "\n",
    "        isinstance - Checks if the current layer is an 'Embedding' (lookup table) layer.\n",
    "\n",
    "        layer weights are filled with small random numbers that are normally disctributed, mean=0, std=0.02        \n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)    \n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        \"\"\"\n",
    "        B - how many sentences the function considers\n",
    "        T - how many letters in each sentence\n",
    "        C - how many possible letters exist (vocab size)\n",
    "\n",
    "        .view takes values and inserts into a tensor\n",
    "        \"\"\"\n",
    "        # logits = self.token_embedding_table(index) # token look-up table, vectors in table contain information about meaning of each token\n",
    "\n",
    "        B,T = index.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) vector with corresponding positioning\n",
    "        x = tok_emb + pos_emb # (B,T,C), combine word meaning and word location\n",
    "        x = self.blocks(x) # transformer blocks attention, word meaning understood in context\n",
    "        x = self.ln_f(x) # normalise matricies\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size), logit predictions for each token\n",
    "        \n",
    "\n",
    "        # end of text edge case\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:    \n",
    "            B, T, C  = logits.shape\n",
    "            logits = logits.view(B*T, C) # predict on character level, not sentence level\n",
    "            targets = targets.view(B*T) # check predictions against true values\n",
    "            loss = F.cross_entropy(logits, targets) # big prediction mistake = big loss\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \"\"\"\n",
    "        function writes new text one token at a time, based on what it has seen so far\n",
    "\n",
    "        the model gives predictions for every token position, so has shape (B,T,C)\n",
    "        \n",
    "        after logits = logits[:,-1,:] has shape (B,C) - only last token prediction is taken for each batch\n",
    "\n",
    "        dim=-1 applies softmax to last dimension of (B,C), so only predicts on C (probability of next token)\n",
    "\n",
    "        we sample the next possible tokens to add randomness - choosing the highest probability every time creates loops\n",
    "        \n",
    "        \"\"\"\n",
    "        # index is the (B,T) array of indicies in the current context\n",
    "        \n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # use only the last 64 characters to predict the next token\n",
    "            index_limiter = index[:, -block_size:]\n",
    "            # get predictions\n",
    "            logits, loss = self.forward(index_limiter)\n",
    "            # focus on only the last time step - only want last token prediction\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get probability of predicted token            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from predicted token distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return index\n",
    "    \n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in m.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d593686-7973-4191-95be-f1b932449d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.264, val loss: 4.263\n",
      "step: 50, train loss: 2.765, val loss: 2.775\n",
      "step: 100, train loss: 2.524, val loss: 2.521\n",
      "step: 150, train loss: 2.422, val loss: 2.429\n",
      "step: 200, train loss: 2.369, val loss: 2.379\n",
      "step: 250, train loss: 2.324, val loss: 2.337\n",
      "step: 300, train loss: 2.288, val loss: 2.304\n",
      "step: 350, train loss: 2.257, val loss: 2.266\n",
      "step: 400, train loss: 2.220, val loss: 2.232\n",
      "step: 450, train loss: 2.175, val loss: 2.191\n",
      "2.1514599323272705\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# create optimiser\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for itr in range(max_iters):\n",
    "    if itr % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {itr}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00041d2a-6741-4a51-87c3-e5069f905007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "The Great Wizard said thatesad to O) ter odr\n",
      "da throsand codnd  aie.\n",
      "Tig\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The Great Wizard said \"\n",
    "\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "# turn off dropout\n",
    "model.eval()\n",
    "\n",
    "generated_indices = m.generate(context, max_new_tokens=50)\n",
    "\n",
    "output_text = decode(generated_indices[0].tolist())\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(output_text)\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc5134-0d0e-4ad7-a32d-fd4ea82c7dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
